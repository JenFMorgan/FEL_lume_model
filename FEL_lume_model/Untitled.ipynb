{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5cd7eca-5940-4d5e-a3e1-72fc750d616d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Genesis_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxopt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Evaluator, VOCS, Xopt\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxopt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_generator\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mGenesis_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mTaperFunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m Genesis4\u001b[38;5;241m.\u001b[39mapply_taper \u001b[38;5;241m=\u001b[39m apply_taper\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Genesis_functions'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from xopt import Evaluator, VOCS, Xopt\n",
    "from xopt.generators import get_generator\n",
    "from Genesis_functions import *\n",
    "from TaperFunctions import *\n",
    "Genesis4.apply_taper = apply_taper\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from lume_model.variables import ScalarInputVariable, ScalarOutputVariable\n",
    "from lume_model.models import TorchModel\n",
    "from lume_model.utils import variables_as_yaml\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from xopt import Evaluator\n",
    "from xopt import VOCS\n",
    "import math\n",
    "from xopt.generators import get_generator \n",
    "from xopt import Xopt\n",
    "\n",
    "#from xopt import AsynchronousXopt as Xopt\n",
    "from Genesis_functions import *\n",
    "\n",
    "from TaperFunctions import *\n",
    "\n",
    "Genesis4.apply_taper = apply_taper\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from botorch.models.transforms.input import AffineInputTransform\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "# Function to set seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "def limit_points_per_range(dataset, step=100, max_points=1000):\n",
    "    \"\"\"\n",
    "    Reduces the number of points in each energy range (in steps of `step`) \n",
    "    to be less than or equal to `max_points`.\n",
    "\n",
    "    Args:\n",
    "    - dataset (pd.DataFrame): The dataset containing an 'energy' column.\n",
    "    - step (int): Step size for dividing the energy range (default: 100).\n",
    "    - max_points (int): Maximum number of points allowed in each range (default: 1000).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The filtered dataset with reduced points per range.\n",
    "    \"\"\"\n",
    "    # Define bins for the energy ranges\n",
    "    bins = range(int(dataset['energy'].min()), int(dataset['energy'].max()) + step, step)\n",
    "    \n",
    "    # Add a new column for the bin each energy belongs to\n",
    "    dataset['energy_bin'] = pd.cut(dataset['energy'], bins=bins, right=False)\n",
    "    \n",
    "    # Limit the number of points per bin\n",
    "    filtered_dataset = dataset.groupby('energy_bin').apply(\n",
    "        lambda x: x.sample(n=min(len(x), max_points), random_state=42)\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # Drop the temporary energy_bin column\n",
    "    filtered_dataset = filtered_dataset.drop(columns=['energy_bin'])\n",
    "    \n",
    "    return filtered_dataset\n",
    "\n",
    "\n",
    "def scale_and_save_transform(features, dump_filename, dataset):\n",
    "    \"\"\"\n",
    "    Scales the provided features in the dataset using AffineInputTransform, saves the transform, \n",
    "    and returns the scaled dataset.\n",
    "    \n",
    "    Args:\n",
    "    - features (list of str): List of feature names to scale.\n",
    "    - dump_filename (str): The filename to save the AffineInputTransform object.\n",
    "    - dataset (pd.DataFrame): The dataset containing the features to scale.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The dataset with scaled features.\n",
    "    \"\"\"\n",
    "    normalizations = {}\n",
    "\n",
    "    # Collect min and max values for each feature\n",
    "    for feature in features:\n",
    "        min_val = dataset[feature].min()\n",
    "        max_val = dataset[feature].max()\n",
    "        scale =  (max_val - min_val) if max_val != min_val else 1.0  # Avoid division by zero\n",
    "        shift = min_val \n",
    "        normalizations[f\"{feature}_scale\"] = scale\n",
    "        normalizations[f\"{feature}_mean\"] = shift\n",
    "\n",
    "    # Combine scaling factors and offsets into tensors\n",
    "    coefficient = torch.tensor([normalizations[f\"{feature}_scale\"] for feature in features])\n",
    "    offset = torch.tensor([normalizations[f\"{feature}_mean\"] for feature in features])\n",
    "\n",
    "\n",
    "    print(coefficient, offset)\n",
    "    # Create a single AffineInputTransform for all features\n",
    "    affine_transform = AffineInputTransform(\n",
    "        len(features), \n",
    "        coefficient=coefficient, \n",
    "        offset=offset\n",
    "    )\n",
    "\n",
    "    # Apply the transform to the dataset\n",
    "    features_data = torch.tensor(dataset[features].values, dtype=torch.float32)  # Convert features to a tensor\n",
    "    scaled_features = affine_transform.transform(features_data)\n",
    "\n",
    "    #scaled_features = affine_transform.forward(features_data)\n",
    "\n",
    "    # Add the scaled features back to the dataset\n",
    "    for i, feature in enumerate(features):\n",
    "        dataset[f'scaled_{feature}'] = scaled_features[:, i].numpy()\n",
    "\n",
    "    # Save the transform for later use\n",
    "  #  joblib.dump(affine_transform, dump_filename)\n",
    "    torch.save(affine_transform, dump_filename)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def load_transform_and_apply(dump_filename, features, dataset):\n",
    "    \"\"\"\n",
    "    Loads the AffineInputTransform from the specified file and uses it to apply the transform (scaling)\n",
    "    to the provided dataset.\n",
    "    \n",
    "    Args:\n",
    "    - dump_filename (str): The filename where the AffineInputTransform object is saved.\n",
    "    - features (list of str): List of feature names to transform.\n",
    "    - dataset (pd.DataFrame): The dataset containing the original features to be transformed.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The dataset with scaled features.\n",
    "    \"\"\"\n",
    "    # Load the saved AffineInputTransform\n",
    "    affine_transform = torch.load(dump_filename)\n",
    "    \n",
    "    # Extract the features from the dataset\n",
    "    feature_data = torch.tensor(dataset[features].values)  # Convert to tensor\n",
    "    \n",
    "    # Apply the forward transform (scaling)\n",
    "    scaled_features = affine_transform.transform(feature_data)\n",
    "\n",
    "    # Add the scaled features back to the dataset\n",
    "    for i, feature in enumerate(features):\n",
    "        dataset[f'scaled_{feature}'] = scaled_features[:, i].numpy()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def apply_inverse_transform(dump_filename, features, dataset):\n",
    "    \"\"\"\n",
    "    Loads the AffineInputTransform from the specified file and uses it to apply the inverse transform (rescaling)\n",
    "    to the provided dataset.\n",
    "    \n",
    "    Args:\n",
    "    - dump_filename (str): The filename where the AffineInputTransform object is saved.\n",
    "    - features (list of str): List of feature names to rescale (apply inverse transform).\n",
    "    - dataset (pd.DataFrame or np.ndarray): The dataset containing the scaled features to rescale.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: The rescaled features in their original scale.\n",
    "    \"\"\"\n",
    "    # Load the saved AffineInputTransform\n",
    "    affine_transform = torch.load(dump_filename)\n",
    "\n",
    "    # Convert dataset to tensor if necessary\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        scaled_features_data = torch.tensor(dataset[[f'scaled_{feature}' for feature in features]].values)  # Convert to tensor\n",
    "    else:\n",
    "        scaled_features_data = torch.tensor(dataset)  # If already a NumPy array or tensor\n",
    "\n",
    "    # Apply the inverse transform to rescale the data\n",
    "    rescaled_features = affine_transform.untransform(scaled_features_data)\n",
    "\n",
    "    return rescaled_features.numpy()  # Return as NumPy array\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "from lume_model.models import TorchModel, TorchModule\n",
    "from lume_model.variables import ScalarInputVariable, ScalarOutputVariable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open('dump_cnsga.yml', 'r') as file:\n",
    "    data_load = yaml.safe_load(file)\n",
    "\n",
    "data = pd.DataFrame(data_load['data'])\n",
    "filtered_data = data[data['xopt_error'] == False]\n",
    "\n",
    "alphax_values = filtered_data.alphax\n",
    "alphay_values = filtered_data.alphay\n",
    "betax_values = filtered_data.betax\n",
    "betay_values = filtered_data.betay\n",
    "emitx_values = filtered_data.emitx\n",
    "emity_values = filtered_data.emity\n",
    "taper_values = filtered_data.taper\n",
    "energy_values = filtered_data.energy\n",
    "bandwidth_values = filtered_data.bandwidth_FWHM\n",
    "peak_intensity=filtered_data.peak_intensity\n",
    "\n",
    "\n",
    "X_test = Xopt.from_file(\"dump_testsdata.yml\")\n",
    "X_test_filter = X_test.data[X_test.data['xopt_error'] == False]\n",
    "\n",
    "alphax_test = X_test_filter.alphax\n",
    "alphay_test = X_test_filter.alphay\n",
    "betax_test = X_test_filter.betax\n",
    "betay_test = X_test_filter.betay\n",
    "taper_test = X_test_filter.taper\n",
    "emitx_test = X_test_filter.emitx\n",
    "emity_test = X_test_filter.emity\n",
    "energy_test = X_test_filter.energy\n",
    "bandwidth_test = X_test_filter.bandwidth_FWHM\n",
    "peak_intensity_test=X_test_filter.peak_intensity\n",
    "# Save test data to GPU tensors\n",
    "inputs_to_nn = torch.tensor([alphax_test, alphay_test, betax_test, betay_test, emitx_test, emity_test, taper_test], dtype=torch.float32).T.to(device)\n",
    "torch.save(inputs_to_nn, 'lume_model/info/test_model_inputs.pt')\n",
    "\n",
    "outputs_to_nn = torch.tensor([energy_test, bandwidth_test,peak_intensity_test], dtype=torch.float32).T.to(device)\n",
    "torch.save(outputs_to_nn, 'lume_model/info/test_model_outputs.pt')\n",
    "\n",
    "vocs = X_test.vocs\n",
    "input_variables = [ScalarInputVariable(name=key, default=(value[0] + value[1]) / 2, value_range=value) for key, value in vocs.variables.items()]\n",
    "output_variables = [ScalarOutputVariable(name=observable) for observable in vocs.observables]\n",
    "variables_as_yaml(input_variables, output_variables, file='./lume_model/sim_variables.yml')\n",
    "\n",
    "\n",
    "dataset =pd.DataFrame({\n",
    "        'alphax': alphax_values,\n",
    "        'alphay': alphay_values,\n",
    "        'betax': betax_values,\n",
    "        'betay': betay_values,\n",
    "        'emitx': emitx_values,\n",
    "        'emity': emity_values,\n",
    "        'taper': taper_values,\n",
    "        'energy': energy_values,\n",
    "        'bandwidth': abs(bandwidth_values),\n",
    "        'peak_intensity':peak_intensity})\n",
    "\n",
    "duplicate_columns = ['alphax', 'alphay', 'betax', 'betay', 'taper', 'emitx', 'emity']\n",
    "\n",
    "\n",
    "\n",
    "with open('../../generatedata_energy/dump_cnsga.yml', 'r') as file:\n",
    "    data_load2 = yaml.safe_load(file)\n",
    "\n",
    "data2 = pd.DataFrame(data_load2['data'])\n",
    "filtered_data2 = data2[data2['xopt_error'] == False]\n",
    "\n",
    "alphax_values2 = filtered_data2.alphax\n",
    "alphay_values2 = filtered_data2.alphay\n",
    "betax_values2 = filtered_data2.betax\n",
    "betay_values2 = filtered_data2.betay\n",
    "emitx_values2 = filtered_data2.emitx\n",
    "emity_values2 = filtered_data2.emity\n",
    "taper_values2 = filtered_data2.taper\n",
    "energy_values2 = filtered_data2.energy\n",
    "bandwidth_values2 = filtered_data2.bandwidth_FWHM\n",
    "peak_intensity2=filtered_data2.peak_intensity\n",
    "\n",
    "dataset2 = pd.DataFrame({\n",
    "        'alphax': alphax_values2,\n",
    "        'alphay': alphay_values2,\n",
    "        'betax': betax_values2,\n",
    "        'betay': betay_values2,\n",
    "        'emitx': emitx_values2,\n",
    "        'emity': emity_values2,\n",
    "        'taper': taper_values2,\n",
    "        'energy': energy_values2,\n",
    "        'bandwidth': abs(bandwidth_values2),\n",
    "        'peak_intensity':peak_intensity})\n",
    "\n",
    "combined_dataset = pd.concat([dataset, dataset2], ignore_index=True)\n",
    "combined_dataset = combined_dataset.drop_duplicates(subset=duplicate_columns, keep='first')  \n",
    "\n",
    "####combined_dataset=dataset\n",
    "\n",
    "combined_dataset = combined_dataset[combined_dataset['bandwidth'] <=150]\n",
    "\n",
    "\n",
    "\n",
    "test_data_sample = combined_dataset.sample(n=100, random_state=42)\n",
    "\n",
    "# Extract features and target for test dat\n",
    "features_columns = ['alphax', 'alphay', 'betax', 'betay', 'emitx', 'emity', 'taper']\n",
    "target_column = ['energy', 'bandwidth', 'peak_intensity']\n",
    "\n",
    "test_features = test_data_sample[features_columns].values\n",
    "test_target = test_data_sample[target_column].values\n",
    "\n",
    "# Convert to tensors\n",
    "inputs_to_nn2 = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
    "outputs_to_nn2 = torch.tensor(test_target, dtype=torch.float32).to(device)\n",
    "\n",
    "# Save the tensors\n",
    "torch.save(inputs_to_nn2, 'lume_model/info/test_model_inputs2.pt')\n",
    "torch.save(outputs_to_nn2, 'lume_model/info/test_model_outputs2.pt')\n",
    "\n",
    "\n",
    "print('combine', len(combined_dataset), 'data1', len(data), 'data2,', len(data2))\n",
    "\n",
    "\n",
    "# Remove the selected data points from the combined dataset\n",
    "combined_dataset = combined_dataset.drop(test_data_sample.index)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Test dataset saved and removed from the combined dataset successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "print(len(combined_dataset), 'before')\n",
    "\n",
    "#combined_dataset = combined_dataset[combined_dataset['energy'] >= 200]\n",
    "#combined_dataset = combined_dataset[combined_dataset['bandwidth'] <=200]\n",
    "#combined_dataset = limit_points_per_range(combined_dataset, step=50, max_points=400)\n",
    "#print(\"Test dataset saved and removed from the combined dataset successfully!\")\n",
    "\n",
    "\n",
    "print('combined_data', len(combined_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d3a6e5-066b-4c02-afdb-910658f03dce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
